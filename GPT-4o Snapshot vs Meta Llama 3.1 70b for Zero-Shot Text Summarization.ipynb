{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "524d3481",
   "metadata": {},
   "source": [
    "## Detailed Article Explaination\n",
    "\n",
    "The detailed code explanation for this article is available at the following link:\n",
    "\n",
    "https://www.daniweb.com/programming/computer-science/tutorials/542292/gpt-4o-snapshot-vs-meta-llama-3-1-70b-for-zero-shot-text-summarization\n",
    "    \n",
    "For my other articles for Daniweb.com, please see this link:\n",
    "\n",
    "https://www.daniweb.com/members/1235222/usmanmalik57\n",
    "\n",
    "Importing and Installing Required Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df974cf",
   "metadata": {},
   "source": [
    "## Installing and Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bea3c804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\usman\\anaconda3\\lib\\site-packages (1.40.6)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from openai) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from openai) (0.25.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from openai) (0.5.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from openai) (2.4.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\usman\\anaconda3\\lib\\site-packages (from openai) (1.2.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from openai) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: certifi in c:\\users\\usman\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
      "Requirement already satisfied: httpcore<0.19.0,>=0.18.0 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (0.18.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.5.0)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.10.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\usman\\anaconda3\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from httpcore<0.19.0,>=0.18.0->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: groq in c:\\users\\usman\\anaconda3\\lib\\site-packages (0.4.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from groq) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from groq) (0.25.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from groq) (2.4.2)\n",
      "Requirement already satisfied: sniffio in c:\\users\\usman\\anaconda3\\lib\\site-packages (from groq) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from groq) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->groq) (3.4)\n",
      "Requirement already satisfied: certifi in c:\\users\\usman\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (2023.7.22)\n",
      "Requirement already satisfied: httpcore<0.19.0,>=0.18.0 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (0.18.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (0.5.0)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->groq) (2.10.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from httpcore<0.19.0,>=0.18.0->httpx<1,>=0.23.0->groq) (0.14.0)\n",
      "Requirement already satisfied: rouge-score in c:\\users\\usman\\anaconda3\\lib\\site-packages (0.1.2)\n",
      "Requirement already satisfied: absl-py in c:\\users\\usman\\anaconda3\\lib\\site-packages (from rouge-score) (2.0.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\usman\\anaconda3\\lib\\site-packages (from rouge-score) (3.8.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\usman\\anaconda3\\lib\\site-packages (from rouge-score) (1.24.3)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from rouge-score) (1.16.0)\n",
      "Requirement already satisfied: click in c:\\users\\usman\\anaconda3\\lib\\site-packages (from nltk->rouge-score) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\usman\\anaconda3\\lib\\site-packages (from nltk->rouge-score) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from nltk->rouge-score) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\usman\\anaconda3\\lib\\site-packages (from nltk->rouge-score) (4.66.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\usman\\anaconda3\\lib\\site-packages (from click->nltk->rouge-score) (0.4.6)\n",
      "Requirement already satisfied: openpyxl in c:\\users\\usman\\anaconda3\\lib\\site-packages (3.1.5)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\usman\\anaconda3\\lib\\site-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\usman\\anaconda3\\lib\\site-packages (2.2.1)\n",
      "Requirement already satisfied: openpyxl in c:\\users\\usman\\anaconda3\\lib\\site-packages (3.1.5)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\usman\\anaconda3\\lib\\site-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai\n",
    "!pip install groq\n",
    "!pip install rouge-score\n",
    "!pip install --upgrade openpyxl\n",
    "!pip install pandas openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ac88d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\usman\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from rouge_score import rouge_scorer\n",
    "from openai import OpenAI\n",
    "from groq import Groq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1c4148",
   "metadata": {},
   "source": [
    "## Importing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c2fa4017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of summaries: 1168.78 characters\n",
      "(1000, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>human_summary</th>\n",
       "      <th>publication</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>theme</th>\n",
       "      <th>content</th>\n",
       "      <th>summary_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>259</td>\n",
       "      <td>17966</td>\n",
       "      <td>President Donald J. Trump’s decision to build ...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Azam Ahmed</td>\n",
       "      <td>2017-01-27</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>politics</td>\n",
       "      <td>MEXICO CITY  —   President Donald J. Trump’s d...</td>\n",
       "      <td>1259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>790</th>\n",
       "      <td>259</td>\n",
       "      <td>18229</td>\n",
       "      <td>Pinault the chief executive of kering suggeste...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Vanessa Friedman</td>\n",
       "      <td>2017-02-03</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>Yet another seismic shift is taking place in F...</td>\n",
       "      <td>993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>259</td>\n",
       "      <td>17751</td>\n",
       "      <td>Senator Bernie Sanders of Vermont, an independ...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Jennifer Steinhauer, John Schwartz, Emmarie Hu...</td>\n",
       "      <td>2017-01-19</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>politics</td>\n",
       "      <td>Democrats complained mightily about their ques...</td>\n",
       "      <td>1028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>17314</td>\n",
       "      <td>Istanbul the Islamic state on Monday issued a ...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Tim Arango</td>\n",
       "      <td>2017-01-03</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>crime</td>\n",
       "      <td>ISTANBUL  —   The Islamic State on Monday issu...</td>\n",
       "      <td>879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>259</td>\n",
       "      <td>17599</td>\n",
       "      <td>Good morning. Here’s what you need to know: • ...</td>\n",
       "      <td>New York Times</td>\n",
       "      <td>Charles McDermid</td>\n",
       "      <td>2017-01-13</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>politics</td>\n",
       "      <td>Good morning.  Here’s what you need to know: •...</td>\n",
       "      <td>891</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0     id                                      human_summary  \\\n",
       "578         259  17966  President Donald J. Trump’s decision to build ...   \n",
       "790         259  18229  Pinault the chief executive of kering suggeste...   \n",
       "390         259  17751  Senator Bernie Sanders of Vermont, an independ...   \n",
       "25            0  17314  Istanbul the Islamic state on Monday issued a ...   \n",
       "265         259  17599  Good morning. Here’s what you need to know: • ...   \n",
       "\n",
       "        publication                                             author  \\\n",
       "578  New York Times                                         Azam Ahmed   \n",
       "790  New York Times                                   Vanessa Friedman   \n",
       "390  New York Times  Jennifer Steinhauer, John Schwartz, Emmarie Hu...   \n",
       "25   New York Times                                         Tim Arango   \n",
       "265  New York Times                                   Charles McDermid   \n",
       "\n",
       "           date    year month          theme  \\\n",
       "578  2017-01-27  2017.0   1.0       politics   \n",
       "790  2017-02-03  2017.0   2.0  entertainment   \n",
       "390  2017-01-19  2017.0   1.0       politics   \n",
       "25   2017-01-03  2017.0   1.0          crime   \n",
       "265  2017-01-13  2017.0   1.0       politics   \n",
       "\n",
       "                                               content  summary_length  \n",
       "578  MEXICO CITY  —   President Donald J. Trump’s d...            1259  \n",
       "790  Yet another seismic shift is taking place in F...             993  \n",
       "390  Democrats complained mightily about their ques...            1028  \n",
       "25   ISTANBUL  —   The Islamic State on Monday issu...             879  \n",
       "265  Good morning.  Here’s what you need to know: •...             891  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Kaggle dataset download link\n",
    "# https://github.com/reddzzz/DataScience_FP/blob/main/dataset.xlsx\n",
    "\n",
    "\n",
    "dataset = pd.read_excel(r\"D:\\Datasets\\dataset.xlsx\")\n",
    "dataset = dataset.sample(frac=1)\n",
    "dataset['summary_length'] = dataset['human_summary'].apply(len)\n",
    "average_length = dataset['summary_length'].mean()\n",
    "print(f\"Average length of summaries: {average_length:.2f} characters\")\n",
    "print(dataset.shape)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1cf1db",
   "metadata": {},
   "source": [
    "## Text Summarization with GPT-4o Snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ff46b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key = os.environ.get('OPENAI_API_KEY'),\n",
    ")\n",
    "\n",
    "# Function to calculate ROUGE scores\n",
    "def calculate_rouge(reference, candidate):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(reference, candidate)\n",
    "    return {key: value.fmeasure for key, value in scores.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "22405213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing article 1.\n",
      "Summarizing article 2.\n",
      "Summarizing article 3.\n",
      "Summarizing article 4.\n",
      "Summarizing article 5.\n",
      "Summarizing article 6.\n",
      "Summarizing article 7.\n",
      "Summarizing article 8.\n",
      "Summarizing article 9.\n",
      "Summarizing article 10.\n",
      "Summarizing article 11.\n",
      "Summarizing article 12.\n",
      "Summarizing article 13.\n",
      "Summarizing article 14.\n",
      "Summarizing article 15.\n",
      "Summarizing article 16.\n",
      "Summarizing article 17.\n",
      "Summarizing article 18.\n",
      "Summarizing article 19.\n",
      "Summarizing article 20.\n",
      "CPU times: total: 391 ms\n",
      "Wall time: 59.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "results = []\n",
    "\n",
    "i = 0\n",
    "\n",
    "for _, row in dataset[:20].iterrows():\n",
    "    article = row['content']\n",
    "    human_summary = row['human_summary']\n",
    "    \n",
    "    i = i + 1\n",
    "    print(f\"Summarizing article {i}.\")\n",
    "    \n",
    "    prompt = f\"Summarize the following article in 1150 characters. The summary should look like human created:\\n\\n{article}\\n\\nSummary:\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model= \"gpt-4o-2024-08-06\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=1150,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    generated_summary = response.choices[0].message.content\n",
    "    rouge_scores = calculate_rouge(human_summary, generated_summary)\n",
    "    \n",
    "    results.append({\n",
    "    'article_id': row.id,\n",
    "    'generated_summary': generated_summary,\n",
    "    'rouge1': rouge_scores['rouge1'],\n",
    "    'rouge2': rouge_scores['rouge2'],\n",
    "    'rougeL': rouge_scores['rougeL']\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d37c742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1    0.386724\n",
      "rouge2    0.100371\n",
      "rougeL    0.187491\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "mean_values = results_df[[\"rouge1\", \"rouge2\", \"rougeL\"]].mean()\n",
    "print(mean_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "71a2308f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_evaluate_summary(article, summary):\n",
    "    prompt = f\"\"\"Evaluate the following summary for the given article. Rate it on a scale of 1-10 for:\n",
    "    1. Completeness: Does it capture all key points?\n",
    "    2. Conciseness: Is it brief and to the point?\n",
    "    3. Coherence: Is it well-structured and easy to understand?\n",
    "\n",
    "    Article: {article}\n",
    "\n",
    "    Summary: {summary}\n",
    "\n",
    "    Provide the ratings as a comma-separated list (completeness,conciseness,coherence).\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model= \"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=100,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    return [float(score) for score in response.choices[0].message.content.strip().split(',')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d3d6e276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article ID: 18009, Scores: [8.0, 9.0, 9.0]\n",
      "Article ID: 17531, Scores: [8.0, 9.0, 9.0]\n",
      "Article ID: 18159, Scores: [8.0, 9.0, 9.0]\n",
      "Article ID: 18450, Scores: [8.0, 7.0, 9.0]\n",
      "Article ID: 17751, Scores: [8.0, 9.0, 8.0]\n",
      "Article ID: 17648, Scores: [8.0, 9.0, 9.0]\n",
      "Article ID: 17937, Scores: [8.0, 9.0, 9.0]\n",
      "Article ID: 17469, Scores: [8.0, 9.0, 9.0]\n",
      "Article ID: 17955, Scores: [8.0, 9.0, 9.0]\n",
      "Article ID: 18298, Scores: [8.0, 9.0, 9.0]\n",
      "Article ID: 17367, Scores: [8.0, 9.0, 9.0]\n",
      "Article ID: 17542, Scores: [8.0, 9.0, 8.0]\n",
      "Article ID: 17606, Scores: [8.0, 9.0, 9.0]\n",
      "Article ID: 17800, Scores: [8.0, 9.0, 9.0]\n",
      "Article ID: 18329, Scores: [8.0, 9.0, 9.0]\n",
      "Article ID: 17887, Scores: [8.0, 9.0, 8.0]\n",
      "Article ID: 18275, Scores: [8.0, 9.0, 8.0]\n",
      "Article ID: 18297, Scores: [8.0, 9.0, 9.0]\n",
      "Article ID: 18274, Scores: [8.0, 9.0, 9.0]\n",
      "Article ID: 18126, Scores: [8.0, 9.0, 9.0]\n"
     ]
    }
   ],
   "source": [
    "scores_dict = {'completeness': [], 'conciseness': [], 'coherence': []}\n",
    "\n",
    "i = 0\n",
    "for _, row in results_df.iterrows():\n",
    "    i = i + 1\n",
    "    # Corrected method to access content by article_id\n",
    "    article = dataset.loc[dataset['id'] == row['article_id'], 'content'].iloc[0]\n",
    "    scores = llm_evaluate_summary(article, row['generated_summary'])\n",
    "    print(f\"Article ID: {row['article_id']}, Scores: {scores}\")\n",
    "\n",
    "    # Store the scores in the dictionary\n",
    "    scores_dict['completeness'].append(scores[0])\n",
    "    scores_dict['conciseness'].append(scores[1])\n",
    "    scores_dict['coherence'].append(scores[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c32a5011",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Completeness</th>\n",
       "      <th>Conciseness</th>\n",
       "      <th>Coherence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.0</td>\n",
       "      <td>8.9</td>\n",
       "      <td>8.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Completeness  Conciseness  Coherence\n",
       "0           8.0          8.9        8.8"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the average scores\n",
    "average_scores = {\n",
    "    'completeness': sum(scores_dict['completeness']) / len(scores_dict['completeness']),\n",
    "    'conciseness': sum(scores_dict['conciseness']) / len(scores_dict['conciseness']),\n",
    "    'coherence': sum(scores_dict['coherence']) / len(scores_dict['coherence']),\n",
    "}\n",
    "\n",
    "# Convert to DataFrame for better visualization (optional)\n",
    "average_scores_df = pd.DataFrame([average_scores])\n",
    "average_scores_df.columns = ['Completeness', 'Conciseness', 'Coherence']\n",
    "\n",
    "# Display the DataFrame\n",
    "average_scores_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848afdde",
   "metadata": {},
   "source": [
    "## Text Summarization with Llama 3.1 70b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a035e47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Groq(\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "117f111f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing article 1.\n",
      "Summarizing article 2.\n",
      "Summarizing article 3.\n",
      "Summarizing article 4.\n",
      "Summarizing article 5.\n",
      "Summarizing article 6.\n",
      "Summarizing article 7.\n",
      "Summarizing article 8.\n",
      "Summarizing article 9.\n",
      "Summarizing article 10.\n",
      "Summarizing article 11.\n",
      "Summarizing article 12.\n",
      "Summarizing article 13.\n",
      "Summarizing article 14.\n",
      "Summarizing article 15.\n",
      "Summarizing article 16.\n",
      "Summarizing article 17.\n",
      "Summarizing article 18.\n",
      "Summarizing article 19.\n",
      "Summarizing article 20.\n",
      "CPU times: total: 281 ms\n",
      "Wall time: 24.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "results = []\n",
    "\n",
    "i = 0\n",
    "\n",
    "for _, row in dataset[:20].iterrows():\n",
    "    article = row['content']\n",
    "    human_summary = row['human_summary']\n",
    "    \n",
    "    i = i + 1\n",
    "    print(f\"Summarizing article {i}.\")\n",
    "    \n",
    "    prompt = f\"Summarize the following article in 1150 characters. The summary should look like human created:\\n\\n{article}\\n\\nSummary:\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "          model=\"llama-3.1-70b-versatile\",\n",
    "          temperature = 0.7,\n",
    "          max_tokens = 1150,\n",
    "          messages=[\n",
    "                {\"role\": \"user\", \"content\":  prompt}\n",
    "            ]\n",
    "    )\n",
    "\n",
    "    generated_summary = response.choices[0].message.content\n",
    "    rouge_scores = calculate_rouge(human_summary, generated_summary)\n",
    "    \n",
    "    results.append({\n",
    "    'article_id': row.id,\n",
    "    'generated_summary': generated_summary,\n",
    "    'rouge1': rouge_scores['rouge1'],\n",
    "    'rouge2': rouge_scores['rouge2'],\n",
    "    'rougeL': rouge_scores['rougeL']\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5345cb94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge1    0.335863\n",
      "rouge2    0.080865\n",
      "rougeL    0.170834\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "mean_values = results_df[[\"rouge1\", \"rouge2\", \"rougeL\"]].mean()\n",
    "print(mean_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "12b1d934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article ID: 18009, Scores: [8.0, 9.0, 9.0]\n",
      "Article ID: 17531, Scores: [8.0, 9.0, 8.0]\n",
      "Article ID: 18159, Scores: [8.0, 9.0, 9.0]\n",
      "Article ID: 18450, Scores: [8.0, 7.0, 9.0]\n",
      "Article ID: 17751, Scores: [8.0, 9.0, 9.0]\n",
      "Article ID: 17648, Scores: [8.0, 9.0, 8.0]\n",
      "Article ID: 17937, Scores: [8.0, 9.0, 9.0]\n",
      "Article ID: 17469, Scores: [8.0, 9.0, 9.0]\n",
      "Article ID: 17955, Scores: [8.0, 9.0, 8.0]\n",
      "Article ID: 18298, Scores: [8.0, 9.0, 9.0]\n",
      "Article ID: 17367, Scores: [8.0, 9.0, 9.0]\n",
      "Article ID: 17542, Scores: [8.0, 9.0, 8.0]\n",
      "Article ID: 17606, Scores: [8.0, 9.0, 9.0]\n",
      "Article ID: 17800, Scores: [8.0, 9.0, 9.0]\n",
      "Article ID: 18329, Scores: [8.0, 9.0, 9.0]\n",
      "Article ID: 17887, Scores: [8.0, 9.0, 8.0]\n",
      "Article ID: 18275, Scores: [8.0, 9.0, 8.0]\n",
      "Article ID: 18297, Scores: [8.0, 9.0, 9.0]\n",
      "Article ID: 18274, Scores: [8.0, 9.0, 8.0]\n",
      "Article ID: 18126, Scores: [8.0, 9.0, 9.0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Completeness</th>\n",
       "      <th>Conciseness</th>\n",
       "      <th>Coherence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.0</td>\n",
       "      <td>8.9</td>\n",
       "      <td>8.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Completeness  Conciseness  Coherence\n",
       "0           8.0          8.9       8.65"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key = os.environ.get('OPENAI_API_KEY'),\n",
    ")\n",
    "\n",
    "scores_dict = {'completeness': [], 'conciseness': [], 'coherence': []}\n",
    "\n",
    "i = 0\n",
    "for _, row in results_df.iterrows():\n",
    "    i = i + 1\n",
    "    # Corrected method to access content by article_id\n",
    "    article = dataset.loc[dataset['id'] == row['article_id'], 'content'].iloc[0]\n",
    "    scores = llm_evaluate_summary(article, row['generated_summary'])\n",
    "    print(f\"Article ID: {row['article_id']}, Scores: {scores}\")\n",
    "\n",
    "    # Store the scores in the dictionary\n",
    "    scores_dict['completeness'].append(scores[0])\n",
    "    scores_dict['conciseness'].append(scores[1])\n",
    "    scores_dict['coherence'].append(scores[2])\n",
    "    \n",
    "# Calculate the average scores\n",
    "average_scores = {\n",
    "    'completeness': sum(scores_dict['completeness']) / len(scores_dict['completeness']),\n",
    "    'conciseness': sum(scores_dict['conciseness']) / len(scores_dict['conciseness']),\n",
    "    'coherence': sum(scores_dict['coherence']) / len(scores_dict['coherence']),\n",
    "}\n",
    "\n",
    "# Convert to DataFrame for better visualization (optional)\n",
    "average_scores_df = pd.DataFrame([average_scores])\n",
    "average_scores_df.columns = ['Completeness', 'Conciseness', 'Coherence']\n",
    "\n",
    "# Display the DataFrame\n",
    "average_scores_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da9831a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

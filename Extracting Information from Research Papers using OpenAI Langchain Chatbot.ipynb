{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a6bfbda",
   "metadata": {},
   "source": [
    "## Detailed article explaination\n",
    "\n",
    "The detailed code explanation for this article is available at the following link:\n",
    "\n",
    "https://www.daniweb.com/programming/computer-science/tutorials/541151/extracting-information-from-research-papers-using-langchain-openai\n",
    "\n",
    "For my other articles for Daniweb.com, please see this link:\n",
    "\n",
    "https://www.daniweb.com/members/1235222/usmanmalik57"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed56fa1a",
   "metadata": {},
   "source": [
    "## Downloading and Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6dbe0140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\usman\\anaconda3\\lib\\site-packages (0.0.348)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from langchain) (6.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from langchain) (1.4.39)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from langchain) (3.8.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from langchain) (0.6.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: langchain-core<0.1,>=0.0.12 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from langchain) (0.0.12)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.63 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from langchain) (0.0.69)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from langchain) (1.24.3)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from langchain) (2.4.2)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from langchain) (8.2.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.2.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.1)\n",
      "Requirement already satisfied: anyio<5,>=3 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from langchain-core<0.1,>=0.0.12->langchain) (3.7.1)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from langchain-core<0.1,>=0.0.12->langchain) (23.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->langchain) (0.5.0)\n",
      "Requirement already satisfied: pydantic-core==2.10.1 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->langchain) (2.10.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from pydantic<3,>=1->langchain) (4.7.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from anyio<5,>=3->langchain-core<0.1,>=0.0.12->langchain) (1.2.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (0.4.3)\n",
      "Requirement already satisfied: openai in c:\\users\\usman\\anaconda3\\lib\\site-packages (0.28.0)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from openai) (2.31.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\usman\\anaconda3\\lib\\site-packages (from openai) (4.65.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\usman\\anaconda3\\lib\\site-packages (from openai) (3.8.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (2023.7.22)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from aiohttp->openai) (22.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from aiohttp->openai) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from aiohttp->openai) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from aiohttp->openai) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from aiohttp->openai) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from aiohttp->openai) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\usman\\anaconda3\\lib\\site-packages (from tqdm->openai) (0.4.6)\n",
      "Requirement already satisfied: PyPDF2 in c:\\users\\usman\\anaconda3\\lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\usman\\anaconda3\\lib\\site-packages (1.7.4)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\usman\\anaconda3\\lib\\site-packages (0.3.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from tiktoken) (2022.7.9)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n",
      "Collecting rich\n",
      "  Obtaining dependency information for rich from https://files.pythonhosted.org/packages/be/be/1520178fa01eabe014b16e72a952b9f900631142ccd03dc36cf93e30c1ce/rich-13.7.0-py3-none-any.whl.metadata\n",
      "  Downloading rich-13.7.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from rich) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from rich) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\usman\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich) (0.1.0)\n",
      "Downloading rich-13.7.0-py3-none-any.whl (240 kB)\n",
      "   ---------------------------------------- 0.0/240.6 kB ? eta -:--:--\n",
      "   ------------- -------------------------- 81.9/240.6 kB 2.2 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 122.9/240.6 kB 1.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 194.6/240.6 kB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 240.6/240.6 kB 1.6 MB/s eta 0:00:00\n",
      "Installing collected packages: rich\n",
      "Successfully installed rich-13.7.0\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain\n",
    "!pip install openai\n",
    "!pip install PyPDF2\n",
    "!pip install faiss-cpu\n",
    "!pip install tiktoken\n",
    "!pip install rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97d31cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c3c0ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a763cb",
   "metadata": {},
   "source": [
    "## Reading and Chunking Text Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f6191e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_reader = PdfReader(r'D:\\Datasets\\1907.11692.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc3c94f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Concatenate\n",
    "\n",
    "pdf_text = ''\n",
    "for i, page in enumerate(pdf_reader.pages):\n",
    "    page_content = page.extract_text()\n",
    "    if page_content:\n",
    "        pdf_text += page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ae0cb97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arXiv:1907.11692v1  [cs.CL]  26 Jul 2019RoBERTa: A Robustly Optimized BERT Pretraining Approach\\nYinhan Liu∗§Myle Ott∗§Naman Goyal∗§Jingfei Du∗§Mandar Joshi†\\nDanqi Chen§Omer Levy§Mike Lewis§Luke Zettlemoyer†§Veselin Stoyanov§\\n†Paul G. Allen School of Computer Science & Engineering,\\nUniversity of Washington, Seattle, WA\\n{mandar90,lsz }@cs.washington.edu\\n§Facebook AI\\n{yinhanliu,myleott,naman,jingfeidu,\\ndanqi,omerlevy,mikelewis,lsz,ves }@fb.com\\nAbstract\\nLanguage model pretraining has led to sig-\\nniﬁcant performance gains but careful com-\\nparison between different approaches is chal-\\nlenging. Training is computationally expen-\\nsive, often done on private datasets of different\\nsizes, and, as we will show, hyperparameter\\nchoices have signiﬁcant impact on the ﬁnal re-\\nsults. We present a replication study of BERT\\npretraining ( Devlin et al. ,2019 ) that carefully\\nmeasures the impact of many key hyperparam-\\neters and training data size. We ﬁnd that BERT\\nwas signiﬁcantly undertrained, and can match\\nor exceed the performance of every model\\npublished after it. Our best model achieves\\nstate-of-the-art results on GLUE, RACE and\\nSQuAD. These results highlight the impor-\\ntance of previously overlooked design choices,\\nand raise questions about the source of re-\\ncently reported improvements. We release our\\nmodels and code.1\\n1 Introduction\\nSelf-training methods such as ELMo ( Peters et al. ,\\n2018 ), GPT ( Radford et al. ,2018 ), BERT\\n(Devlin et al. ,2019 ), XLM ( Lample and Conneau ,\\n2019 ), and XLNet ( Yang et al. ,2019 ) have\\nbrought signiﬁcant performance gains, but it can\\nbe challenging to determine which aspects of\\nthe methods contribute the most. Training is\\ncomputationally expensive, limiting the amount\\nof tuning that can be done, and is often done with\\nprivate training data of varying sizes, limiting\\nour ability to measure the effects of the modeling\\nadvances.\\n∗Equal contribution.\\n1Our models and code are available at:\\nhttps://github.com/pytorch/fairseqWe present a replication study of BERT pre-\\ntraining ( Devlin et al. ,2019 ), which includes a\\ncareful evaluation of the effects of hyperparmeter\\ntuning and training set size. We ﬁnd that BERT\\nwas signiﬁcantly undertrained and propose an im-\\nproved recipe for training BERT models, which\\nwe call RoBERTa, that can match or exceed the\\nperformance of all of the post-BERT methods.\\nOur modiﬁcations are simple, they include: (1)\\ntraining the model longer, with bigger batches,\\nover more data; (2) removing the next sentence\\nprediction objective; (3) training on longer se-\\nquences; and (4) dynamically changing the mask-\\ning pattern applied to the training data. We also\\ncollect a large new dataset (CC-N EWS) of compa-\\nrable size to other privately used datasets, to better\\ncontrol for training set size effects.\\nWhen controlling for training data, our im-\\nproved training procedure improves upon the pub-\\nlished BERT results on both GLUE and SQuAD.\\nWhen trained for longer over additional data, our\\nmodel achieves a score of 88.5 on the public\\nGLUE leaderboard, matching the 88.4 reported\\nbyYang et al. (2019 ). Our model establishes a\\nnew state-of-the-art on 4/9 of the GLUE tasks:\\nMNLI, QNLI, RTE and STS-B. We also match\\nstate-of-the-art results on SQuAD and RACE.\\nOverall, we re-establish that BERT’s masked lan-\\nguage model training objective is competitive\\nwith other recently proposed training objectives\\nsuch as perturbed autoregressive language model-\\ning (Yang et al. ,2019 ).2\\nIn summary, the contributions of this paper\\nare: (1) We present a set of important BERT de-\\nsign choices and training strategies and introduce\\n2It is possible that these other methods could also improve\\nwith more tuning. We leave this exploration to future work.alternatives that lead to better downstream task\\nperformance; (2) We use a novel dataset, CC-\\nNEWS, and conﬁrm that using more data for pre-\\ntraining further improves performance on down-\\nstream tasks; (3) Our training improvements show\\nthat masked language model pretraining, under\\nthe right design choices, is competitive with all\\nother recently published methods. We release our\\nmodel, pretraining and ﬁne-tuning code imple-\\nmented in PyTorch ( Paszke et al. ,2017 ).\\n2 Background\\nIn this section, we give a brief overview of the\\nBERT ( Devlin et al. ,2019 ) pretraining approach\\nand some of the training choices that we will ex-\\namine experimentally in the following section.\\n2.1 Setup\\nBERT takes as input a concatenation of two\\nsegments (sequences of tokens), x1,...,x N\\nandy1,...,yM. Segments usually consist of\\nmore than one natural sentence. The two seg-\\nments are presented as a single input sequence\\nto BERT with special tokens delimiting them:\\n[CLS],x1,...,x N,[SEP],y1,...,yM,[EOS].\\nMandNare constrained such that M+N < T ,\\nwhereTis a parameter that controls the maximum\\nsequence length during training.\\nThe model is ﬁrst pretrained on a large unla-\\nbeled text corpus and subsequently ﬁnetuned us-\\ning end-task labeled data.\\n2.2 Architecture\\nBERT uses the now ubiquitous transformer archi-\\ntecture ( Vaswani et al. ,2017 ), which we will not\\nreview in detail. We use a transformer architecture\\nwithLlayers. Each block uses Aself-attention\\nheads and hidden dimension H.\\n2.3 Training Objectives\\nDuring pretraining, BERT uses two objectives:\\nmasked language modeling and next sentence pre-\\ndiction.\\nMasked Language Model (MLM) A random\\nsample of the tokens in the input sequence is\\nselected and replaced with the special token\\n[MASK]. The MLM objective is a cross-entropy\\nloss on predicting the masked tokens. BERT uni-\\nformly selects 15% of the input tokens for possi-\\nble replacement. Of the selected tokens, 80% are\\nreplaced with [MASK], 10% are left unchanged,and 10% are replaced by a randomly selected vo-\\ncabulary token.\\nIn the original implementation, random mask-\\ning and replacement is performed once in the be-\\nginning and saved for the duration of training, al-\\nthough in practice, data is duplicated so the mask\\nis not always the same for every training sentence\\n(see Section 4.1).\\nNext Sentence Prediction (NSP) NSP is a bi-\\nnary classiﬁcation loss for predicting whether two\\nsegments follow each other in the original text.\\nPositive examples are created by taking consecu-\\ntive sentences from the text corpus. Negative ex-\\namples are created by pairing segments from dif-\\nferent documents. Positive and negative examples\\nare sampled with equal probability.\\nThe NSP objective was designed to improve\\nperformance on downstream tasks, such as Natural\\nLanguage Inference ( Bowman et al. ,2015 ), which\\nrequire reasoning about the relationships between\\npairs of sentences.\\n2.4 Optimization\\nBERT is optimized with Adam ( Kingma and Ba ,\\n2015 ) using the following parameters: β1= 0.9,\\nβ2= 0.999,ǫ=1e-6 and L2weight de-\\ncay of0.01. The learning rate is warmed up\\nover the ﬁrst 10,000 steps to a peak value of\\n1e-4, and then linearly decayed. BERT trains\\nwith a dropout of 0.1 on all layers and at-\\ntention weights, and a GELU activation func-\\ntion ( Hendrycks and Gimpel ,2016 ). Models are\\npretrained for S=1,000,000 updates, with mini-\\nbatches containing B=256 sequences of maxi-\\nmum length T=512 tokens.\\n2.5 Data\\nBERT is trained on a combination of B OOK COR-\\nPUS (Zhu et al. ,2015 ) plus English W IKIPEDIA ,\\nwhich totals 16GB of uncompressed text.3\\n3 Experimental Setup\\nIn this section, we describe the experimental setup\\nfor our replication study of BERT.\\n3.1 Implementation\\nWe reimplement BERT in FAIRSEQ (Ott et al. ,\\n2019 ). We primarily follow the original BERT\\n3Yang et al. (2019 ) use the same dataset but report having\\nonly 13GB of text after data cleaning. This is most likely due\\nto subtle differences in cleaning of the Wikipedia data.optimization hyperparameters, given in Section 2,\\nexcept for the peak learning rate and number of\\nwarmup steps, which are tuned separately for each\\nsetting. We additionally found training to be very\\nsensitive to the Adam epsilon term, and in some\\ncases we obtained better performance or improved\\nstability after tuning it. Similarly, we found setting\\nβ2= 0.98to improve stability when training with\\nlarge batch sizes.\\nWe pretrain with sequences of at most T= 512\\ntokens. Unlike Devlin et al. (2019 ), we do not ran-\\ndomly inject short sequences, and we do not train\\nwith a reduced sequence length for the ﬁrst 90% of\\nupdates. We train only with full-length sequences.\\nWe train with mixed precision ﬂoating point\\narithmetic on DGX-1 machines, each with 8 ×\\n32GB Nvidia V100 GPUs interconnected by In-\\nﬁniband ( Micikevicius et al. ,2018 ).\\n3.2 Data\\nBERT-style pretraining crucially relies on large\\nquantities of text. Baevski et al. (2019 ) demon-\\nstrate that increasing data size can result in im-\\nproved end-task performance. Several efforts\\nhave trained on datasets larger and more diverse\\nthan the original BERT ( Radford et al. ,2019 ;\\nYang et al. ,2019 ;Zellers et al. ,2019 ). Unfortu-\\nnately, not all of the additional datasets can be\\npublicly released. For our study, we focus on gath-\\nering as much data as possible for experimenta-\\ntion, allowing us to match the overall quality and\\nquantity of data as appropriate for each compari-\\nson.\\nWe consider ﬁve English-language corpora of\\nvarying sizes and domains, totaling over 160GB\\nof uncompressed text. We use the following text\\ncorpora:\\n•BOOK CORPUS (Zhu et al. ,2015 ) plus English\\nWIKIPEDIA . This is the original data used to\\ntrain BERT. (16GB).\\n•CC-N EWS, which we collected from the En-\\nglish portion of the CommonCrawl News\\ndataset ( Nagel ,2016 ). The data contains 63\\nmillion English news articles crawled between\\nSeptember 2016 and February 2019. (76GB af-\\nter ﬁltering).4\\n•OPENWEBTEXT (Gokaslan and Cohen ,2019 ),\\nan open-source recreation of the WebText cor-\\n4We usenews-please (Hamborg et al. ,2017 ) to col-\\nlect and extract CC-N EWS. CC-N EWS is similar to the R E-\\nALNEWS dataset described in Zellers et al. (2019 ).pus described in Radford et al. (2019 ). The text\\nis web content extracted from URLs shared on\\nReddit with at least three upvotes. (38GB).5\\n•STORIES , a dataset introduced in Trinh and Le\\n(2018 ) containing a subset of CommonCrawl\\ndata ﬁltered to match the story-like style of\\nWinograd schemas. (31GB).\\n3.3 Evaluation\\nFollowing previous work, we evaluate our pre-\\ntrained models on downstream tasks using the fol-\\nlowing three benchmarks.\\nGLUE The General Language Understand-\\ning Evaluation (GLUE) benchmark ( Wang et al. ,\\n2019b ) is a collection of 9 datasets for evaluating\\nnatural language understanding systems.6Tasks\\nare framed as either single-sentence classiﬁcation\\nor sentence-pair classiﬁcation tasks. The GLUE\\norganizers provide training and development data\\nsplits as well as a submission server and leader-\\nboard that allows participants to evaluate and com-\\npare their systems on private held-out test data.\\nFor the replication study in Section 4, we report\\nresults on the development sets after ﬁnetuning\\nthe pretrained models on the corresponding single-\\ntask training data (i.e., without multi-task training\\nor ensembling). Our ﬁnetuning procedure follows\\nthe original BERT paper ( Devlin et al. ,2019 ).\\nIn Section 5we additionally report test set re-\\nsults obtained from the public leaderboard. These\\nresults depend on a several task-speciﬁc modiﬁca-\\ntions, which we describe in Section 5.1.\\nSQuAD The Stanford Question Answering\\nDataset (SQuAD) provides a paragraph of context\\nand a question. The task is to answer the question\\nby extracting the relevant span from the context.\\nWe evaluate on two versions of SQuAD: V1.1\\nand V2.0 ( Rajpurkar et al. ,2016 ,2018 ). In V1.1\\nthe context always contains an answer, whereas in\\n5The authors and their afﬁliated institutions are not in any\\nway afﬁliated with the creation of the OpenWebText dataset.\\n6The datasets are: CoLA ( Warstadt et al. ,2018 ),\\nStanford Sentiment Treebank (SST) ( Socher et al. ,\\n2013 ), Microsoft Research Paragraph Corpus\\n(MRPC) ( Dolan and Brockett ,2005 ), Semantic Tex-\\ntual Similarity Benchmark (STS) ( Agirre et al. ,2007 ),\\nQuora Question Pairs (QQP) ( Iyer et al. ,2016 ), Multi-\\nGenre NLI (MNLI) ( Williams et al. ,2018 ), Question NLI\\n(QNLI) ( Rajpurkar et al. ,2016 ), Recognizing Textual\\nEntailment (RTE) ( Dagan et al. ,2006 ;Bar-Haim et al. ,\\n2006 ;Giampiccolo et al. ,2007 ;Bentivogli et al. ,2009 ) and\\nWinograd NLI (WNLI) ( Levesque et al. ,2011 ).V2.0 some questions are not answered in the pro-\\nvided context, making the task more challenging.\\nFor SQuAD V1.1 we adopt the same span pre-\\ndiction method as BERT ( Devlin et al. ,2019 ). For\\nSQuAD V2.0, we add an additional binary classi-\\nﬁer to predict whether the question is answerable,\\nwhich we train jointly by summing the classiﬁca-\\ntion and span loss terms. During evaluation, we\\nonly predict span indices on pairs that are classi-\\nﬁed as answerable.\\nRACE The ReAding Comprehension from Ex-\\naminations (RACE) ( Lai et al. ,2017 ) task is a\\nlarge-scale reading comprehension dataset with\\nmore than 28,000 passages and nearly 100,000\\nquestions. The dataset is collected from English\\nexaminations in China, which are designed for\\nmiddle and high school students. In RACE, each\\npassage is associated with multiple questions. For\\nevery question, the task is to select one correct an-\\nswer from four options. RACE has signiﬁcantly\\nlonger context than other popular reading compre-\\nhension datasets and the proportion of questions\\nthat requires reasoning is very large.\\n4 Training Procedure Analysis\\nThis section explores and quantiﬁes which choices\\nare important for successfully pretraining BERT\\nmodels. We keep the model architecture ﬁxed.7\\nSpeciﬁcally, we begin by training BERT models\\nwith the same conﬁguration as BERT BASE (L=\\n12,H= 768 ,A= 12 , 110M params).\\n4.1 Static vs. Dynamic Masking\\nAs discussed in Section 2, BERT relies on ran-\\ndomly masking and predicting tokens. The orig-\\ninal BERT implementation performed masking\\nonce during data preprocessing, resulting in a sin-\\nglestatic mask. To avoid using the same mask for\\neach training instance in every epoch, training data\\nwas duplicated 10 times so that each sequence is\\nmasked in 10 different ways over the 40 epochs of\\ntraining. Thus, each training sequence was seen\\nwith the same mask four times during training.\\nWe compare this strategy with dynamic mask-\\ningwhere we generate the masking pattern every\\ntime we feed a sequence to the model. This be-\\ncomes crucial when pretraining for more steps or\\nwith larger datasets.\\n7Studying architectural changes, including larger archi-\\ntectures, is an important area for future work.Masking SQuAD 2.0 MNLI-m SST-2\\nreference 76.3 84.3 92.8\\nOur reimplementation:\\nstatic 78.3 84.3 92.5\\ndynamic 78.7 84.0 92.9\\nTable 1: Comparison between static and dynamic\\nmasking for BERT BASE. We report F1 for SQuAD and\\naccuracy for MNLI-m and SST-2. Reported results are\\nmedians over 5 random initializations (seeds). Refer-\\nence results are from Yang et al. (2019 ).\\nResults Table 1compares the published\\nBERT BASE results from Devlin et al. (2019 ) to our\\nreimplementation with either static or dynamic\\nmasking. We ﬁnd that our reimplementation\\nwith static masking performs similar to the\\noriginal BERT model, and dynamic masking is\\ncomparable or slightly better than static masking.\\nGiven these results and the additional efﬁciency\\nbeneﬁts of dynamic masking, we use dynamic\\nmasking in the remainder of the experiments.\\n4.2 Model Input Format and Next Sentence\\nPrediction\\nIn the original BERT pretraining procedure, the\\nmodel observes two concatenated document seg-\\nments, which are either sampled contiguously\\nfrom the same document (with p= 0.5) or from\\ndistinct documents. In addition to the masked lan-\\nguage modeling objective, the model is trained to\\npredict whether the observed document segments\\ncome from the same or distinct documents via an\\nauxiliary Next Sentence Prediction (NSP) loss.\\nThe NSP loss was hypothesized to be an impor-\\ntant factor in training the original BERT model.\\nDevlin et al. (2019 ) observe that removing NSP\\nhurts performance, with signiﬁcant performance\\ndegradation on QNLI, MNLI, and SQuAD 1.1.\\nHowever, some recent work has questioned the\\nnecessity of the NSP loss ( Lample and Conneau ,\\n2019 ;Yang et al. ,2019 ;Joshi et al. ,2019 ).\\nTo better understand this discrepancy, we com-\\npare several alternative training formats:\\n•SEGMENT -PAIR +NSP: This follows the original\\ninput format used in BERT ( Devlin et al. ,2019 ),\\nwith the NSP loss. Each input has a pair of seg-\\nments, which can each contain multiple natural\\nsentences, but the total combined length must\\nbe less than 512 tokens.Model SQuAD 1.1/2.0 MNLI-m SST-2 RACE\\nOur reimplementation (with NSP loss):\\nSEGMENT -PAIR 90.4/78.7 84.0 92.9 64.2\\nSENTENCE -PAIR 88.7/76.2 82.9 92.1 63.0\\nOur reimplementation (without NSP loss):\\nFULL -SENTENCES 90.4/79.1 84.7 92.5 64.8\\nDOC-SENTENCES 90.6/79.7 84.7 92.7 65.6\\nBERT BASE 88.5/76.3 84.3 92.8 64.3\\nXLNet BASE (K = 7) –/81.3 85.8 92.7 66.1\\nXLNet BASE (K = 6) –/81.0 85.6 93.4 66.7\\nTable 2: Development set results for base models pretrained over B OOK CORPUS and W IKIPEDIA . All models are\\ntrained for 1M steps with a batch size of 256 sequences. We rep ort F1 for SQuAD and accuracy for MNLI-m,\\nSST-2 and RACE. Reported results are medians over ﬁve random initializations (seeds). Results for BERT BASEand\\nXLNet BASEare from Yang et al. (2019 ).\\n•SENTENCE -PAIR +NSP: Each input contains a\\npair of natural sentences , either sampled from\\na contiguous portion of one document or from\\nseparate documents. Since these inputs are sig-\\nniﬁcantly shorter than 512 tokens, we increase\\nthe batch size so that the total number of tokens\\nremains similar to SEGMENT -PAIR +NSP. We re-\\ntain the NSP loss.\\n•FULL -SENTENCES : Each input is packed with\\nfull sentences sampled contiguously from one\\nor more documents, such that the total length is\\nat most 512 tokens. Inputs may cross document\\nboundaries. When we reach the end of one doc-\\nument, we begin sampling sentences from the\\nnext document and add an extra separator token\\nbetween documents. We remove the NSP loss.\\n•DOC-SENTENCES : Inputs are constructed sim-\\nilarly to FULL -SENTENCES , except that they\\nmay not cross document boundaries. Inputs\\nsampled near the end of a document may be\\nshorter than 512 tokens, so we dynamically in-\\ncrease the batch size in these cases to achieve\\na similar number of total tokens as FULL -\\nSENTENCES . We remove the NSP loss.\\nResults Table 2shows results for the four dif-\\nferent settings. We ﬁrst compare the original\\nSEGMENT -PAIR input format from Devlin et al.\\n(2019 ) to the SENTENCE -PAIR format; both for-\\nmats retain the NSP loss, but the latter uses sin-\\ngle sentences. We ﬁnd that using individual\\nsentences hurts performance on downstream\\ntasks , which we hypothesize is because the model\\nis not able to learn long-range dependencies.We next compare training without the NSP\\nloss and training with blocks of text from a sin-\\ngle document ( DOC-SENTENCES ). We ﬁnd that\\nthis setting outperforms the originally published\\nBERT BASEresults and that removing the NSP loss\\nmatches or slightly improves downstream task\\nperformance , in contrast to Devlin et al. (2019 ).\\nIt is possible that the original BERT implementa-\\ntion may only have removed the loss term while\\nstill retaining the SEGMENT -PAIR input format.\\nFinally we ﬁnd that restricting sequences to\\ncome from a single document ( DOC-SENTENCES )\\nperforms slightly better than packing sequences\\nfrom multiple documents ( FULL -SENTENCES ).\\nHowever, because the DOC-SENTENCES format\\nresults in variable batch sizes, we use FULL -\\nSENTENCES in the remainder of our experiments\\nfor easier comparison with related work.\\n4.3 Training with large batches\\nPast work in Neural Machine Translation has\\nshown that training with very large mini-batches\\ncan both improve optimization speed and end-task\\nperformance when the learning rate is increased\\nappropriately ( Ott et al. ,2018 ). Recent work has\\nshown that BERT is also amenable to large batch\\ntraining ( You et al. ,2019 ).\\nDevlin et al. (2019 ) originally trained\\nBERT BASE for 1M steps with a batch size of\\n256 sequences. This is equivalent in computa-\\ntional cost, via gradient accumulation, to training\\nfor 125K steps with a batch size of 2K sequences,\\nor for 31K steps with a batch size of 8K.\\nIn Table 3we compare perplexity and end-bsz steps lr ppl MNLI-m SST-2\\n256 1M 1e-4 3.99 84.7 92.7\\n2K 125K 7e-4 3.68 85.2 92.9\\n8K 31K 1e-3 3.77 84.6 92.8\\nTable 3: Perplexity on held-out training data ( ppl) and\\ndevelopment set accuracy for base models trained over\\nBOOK CORPUS and W IKIPEDIA with varying batch\\nsizes ( bsz). We tune the learning rate ( lr) for each set-\\nting. Models make the same number of passes over the\\ndata (epochs) and have the same computational cost.\\ntask performance of BERT BASE as we increase the\\nbatch size, controlling for the number of passes\\nthrough the training data. We observe that train-\\ning with large batches improves perplexity for the\\nmasked language modeling objective, as well as\\nend-task accuracy. Large batches are also easier to\\nparallelize via distributed data parallel training,8\\nand in later experiments we train with batches of\\n8K sequences.\\nNotably You et al. (2019 ) train BERT with even\\nlarger batche sizes, up to 32K sequences. We leave\\nfurther exploration of the limits of large batch\\ntraining to future work.\\n4.4 Text Encoding\\nByte-Pair Encoding (BPE) ( Sennrich et al. ,2016 )\\nis a hybrid between character- and word-level rep-\\nresentations that allows handling the large vocab-\\nularies common in natural language corpora. In-\\nstead of full words, BPE relies on subwords units,\\nwhich are extracted by performing statistical anal-\\nysis of the training corpus.\\nBPE vocabulary sizes typically range from\\n10K-100K subword units. However, unicode char-\\nacters can account for a sizeable portion of this\\nvocabulary when modeling large and diverse cor-\\npora, such as the ones considered in this work.\\nRadford et al. (2019 ) introduce a clever imple-\\nmentation of BPE that uses bytes instead of uni-\\ncode characters as the base subword units. Using\\nbytes makes it possible to learn a subword vocab-\\nulary of a modest size (50K units) that can still en-\\ncode any input text without introducing any “un-\\nknown” tokens.\\n8Large batch training can improve training efﬁciency even\\nwithout large scale parallel hardware through gradient ac-\\ncumulation , whereby gradients from multiple mini-batches\\nare accumulated locally before each optimization step. Thi s\\nfunctionality is supported natively in FAIRSEQ (Ott et al. ,\\n2019 ).The original BERT implementa-\\ntion ( Devlin et al. ,2019 ) uses a character-level\\nBPE vocabulary of size 30K, which is learned\\nafter preprocessing the input with heuristic tok-\\nenization rules. Following Radford et al. (2019 ),\\nwe instead consider training BERT with a larger\\nbyte-level BPE vocabulary containing 50K sub-\\nword units, without any additional preprocessing\\nor tokenization of the input. This adds approxi-\\nmately 15M and 20M additional parameters for\\nBERT BASEand BERT LARGE , respectively.\\nEarly experiments revealed only slight dif-\\nferences between these encodings, with the\\nRadford et al. (2019 ) BPE achieving slightly\\nworse end-task performance on some tasks. Nev-\\nertheless, we believe the advantages of a univer-\\nsal encoding scheme outweighs the minor degre-\\ndation in performance and use this encoding in\\nthe remainder of our experiments. A more de-\\ntailed comparison of these encodings is left to fu-\\nture work.\\n5 RoBERTa\\nIn the previous section we propose modiﬁcations\\nto the BERT pretraining procedure that improve\\nend-task performance. We now aggregate these\\nimprovements and evaluate their combined im-\\npact. We call this conﬁguration RoBERTa for\\nRobustly optimized BERT approach. Speciﬁ-\\ncally, RoBERTa is trained with dynamic mask-\\ning (Section 4.1),FULL -SENTENCES without NSP\\nloss (Section 4.2), large mini-batches (Section 4.3)\\nand a larger byte-level BPE (Section 4.4).\\nAdditionally, we investigate two other impor-\\ntant factors that have been under-emphasized in\\nprevious work: (1) the data used for pretraining,\\nand (2) the number of training passes through the\\ndata. For example, the recently proposed XLNet\\narchitecture ( Yang et al. ,2019 ) is pretrained us-\\ning nearly 10 times more data than the original\\nBERT ( Devlin et al. ,2019 ). It is also trained with\\na batch size eight times larger for half as many op-\\ntimization steps, thus seeing four times as many\\nsequences in pretraining compared to BERT.\\nTo help disentangle the importance of these fac-\\ntors from other modeling choices (e.g., the pre-\\ntraining objective), we begin by training RoBERTa\\nfollowing the BERT LARGE architecture ( L= 24 ,\\nH= 1024 ,A= 16 , 355M parameters). We\\npretrain for 100K steps over a comparable B OOK -\\nCORPUS plus W IKIPEDIA dataset as was used inModel data bsz stepsSQuADMNLI-m SST-2(v1.1/2.0)\\nRoBERTa\\nwith B OOKS + W IKI 16GB 8K 100K 93.6/87.3 89.0 95.3\\n+ additional data ( §3.2) 160GB 8K 100K 94.0/87.7 89.3 95.6\\n+ pretrain longer 160GB 8K 300K 94.4/88.7 90.0 96.1\\n+ pretrain even longer 160GB 8K 500K 94.6/89.4 90.2 96.4\\nBERT LARGE\\nwith B OOKS + W IKI 13GB 256 1M 90.9/81.8 86.6 93.7\\nXLNet LARGE\\nwith B OOKS + W IKI 13GB 256 1M 94.0/87.8 88.4 94.4\\n+ additional data 126GB 2K 500K 94.5/88.8 89.8 95.6\\nTable 4: Development set results for RoBERTa as we pretrain o ver more data (16GB →160GB of text) and pretrain\\nfor longer (100K →300K→500K steps). Each row accumulates improvements from the row s above. RoBERTa\\nmatches the architecture and training objective of BERT LARGE . Results for BERT LARGE and XLNet LARGE are from\\nDevlin et al. (2019 ) and Yang et al. (2019 ), respectively. Complete results on all GLUE tasks can be fo und in the\\nAppendix.\\nDevlin et al. (2019 ). We pretrain our model using\\n1024 V100 GPUs for approximately one day.\\nResults We present our results in Table 4. When\\ncontrolling for training data, we observe that\\nRoBERTa provides a large improvement over the\\noriginally reported BERT LARGE results, reafﬁrming\\nthe importance of the design choices we explored\\nin Section 4.\\nNext, we combine this data with the three ad-\\nditional datasets described in Section 3.2. We\\ntrain RoBERTa over the combined data with the\\nsame number of training steps as before (100K).\\nIn total, we pretrain over 160GB of text. We ob-\\nserve further improvements in performance across\\nall downstream tasks, validating the importance of\\ndata size and diversity in pretraining.9\\nFinally, we pretrain RoBERTa for signiﬁcantly\\nlonger, increasing the number of pretraining steps\\nfrom 100K to 300K, and then further to 500K. We\\nagain observe signiﬁcant gains in downstream task\\nperformance, and the 300K and 500K step mod-\\nels outperform XLNet LARGE across most tasks. We\\nnote that even our longest-trained model does not\\nappear to overﬁt our data and would likely beneﬁt\\nfrom additional training.\\nIn the rest of the paper, we evaluate our best\\nRoBERTa model on the three different bench-\\nmarks: GLUE, SQuaD and RACE. Speciﬁcally\\n9Our experiments conﬂate increases in data size and di-\\nversity. We leave a more careful analysis of these two dimen-\\nsions to future work.we consider RoBERTa trained for 500K steps over\\nall ﬁve of the datasets introduced in Section 3.2.\\n5.1 GLUE Results\\nFor GLUE we consider two ﬁnetuning settings.\\nIn the ﬁrst setting ( single-task, dev ) we ﬁnetune\\nRoBERTa separately for each of the GLUE tasks,\\nusing only the training data for the correspond-\\ning task. We consider a limited hyperparameter\\nsweep for each task, with batch sizes ∈ {16,32}\\nand learning rates ∈ {1e−5,2e−5,3e−5}, with a\\nlinear warmup for the ﬁrst 6% of steps followed by\\na linear decay to 0. We ﬁnetune for 10 epochs and\\nperform early stopping based on each task’s eval-\\nuation metric on the dev set. The rest of the hyper-\\nparameters remain the same as during pretraining.\\nIn this setting, we report the median development\\nset results for each task over ﬁve random initial-\\nizations, without model ensembling.\\nIn the second setting ( ensembles, test ), we com-\\npare RoBERTa to other approaches on the test set\\nvia the GLUE leaderboard. While many submis-\\nsions to the GLUE leaderboard depend on multi-\\ntask ﬁnetuning, our submission depends only on\\nsingle-task ﬁnetuning . For RTE, STS and MRPC\\nwe found it helpful to ﬁnetune starting from the\\nMNLI single-task model, rather than the baseline\\npretrained RoBERTa. We explore a slightly wider\\nhyperparameter space, described in the Appendix,\\nand ensemble between 5 and 7 models per task.MNLI QNLI QQP RTE SST MRPC CoLA STS WNLI Avg\\nSingle-task single models on dev\\nBERT LARGE 86.6/- 92.3 91.3 70.4 93.2 88.0 60.6 90.0 - -\\nXLNet LARGE 89.8/- 93.9 91.8 83.8 95.6 89.2 63.6 91.8 - -\\nRoBERTa 90.2/90.2 94.7 92.2 86.6 96.4 90.9 68.0 92.4 91.3 -\\nEnsembles on test (from leaderboard as of July 25, 2019)\\nALICE 88.2/87.9 95.7 90.7 83.5 95.2 92.6 68.6 91.1 80.8 86.3\\nMT-DNN 87.9/87.4 96.0 89.9 86.3 96.5 92.7 68.4 91.1 89.0 87.6\\nXLNet 90.2/89.8 98.6 90.3 86.3 96.8 93.0 67.8 91.6 90.4 88.4\\nRoBERTa 90.8/90.2 98.9 90.2 88.2 96.7 92.3 67.8 92.2 89.0 88.5\\nTable 5: Results on GLUE. All results are based on a 24-layer a rchitecture. BERT LARGE and XLNet LARGE results\\nare from Devlin et al. (2019 ) and Yang et al. (2019 ), respectively. RoBERTa results on the development set are a\\nmedian over ﬁve runs. RoBERTa results on the test set are ense mbles of single-task models. For RTE, STS and\\nMRPC we ﬁnetune starting from the MNLI model instead of the ba seline pretrained model. Averages are obtained\\nfrom the GLUE leaderboard.\\nTask-speciﬁc modiﬁcations Two of the GLUE\\ntasks require task-speciﬁc ﬁnetuning approaches\\nto achieve competitive leaderboard results.\\nQNLI : Recent submissions on the GLUE\\nleaderboard adopt a pairwise ranking formulation\\nfor the QNLI task, in which candidate answers\\nare mined from the training set and compared to\\none another, and a single (question, candidate)\\npair is classiﬁed as positive ( Liu et al. ,2019b ,a;\\nYang et al. ,2019 ). This formulation signiﬁcantly\\nsimpliﬁes the task, but is not directly comparable\\nto BERT ( Devlin et al. ,2019 ). Following recent\\nwork, we adopt the ranking approach for our test\\nsubmission, but for direct comparison with BERT\\nwe report development set results based on a pure\\nclassiﬁcation approach.\\nWNLI : We found the provided NLI-format\\ndata to be challenging to work with. Instead\\nwe use the reformatted WNLI data from Super-\\nGLUE ( Wang et al. ,2019a ), which indicates the\\nspan of the query pronoun and referent. We ﬁne-\\ntune RoBERTa using the margin ranking loss from\\nKocijan et al. (2019 ). For a given input sentence,\\nwe use spaCy ( Honnibal and Montani ,2017 ) to\\nextract additional candidate noun phrases from the\\nsentence and ﬁnetune our model so that it assigns\\nhigher scores to positive referent phrases than for\\nany of the generated negative candidate phrases.\\nOne unfortunate consequence of this formulation\\nis that we can only make use of the positive train-\\ning examples, which excludes over half of the pro-\\nvided training examples.10\\n10While we only use the provided WNLI training data, ourResults We present our results in Table 5. In the\\nﬁrst setting ( single-task, dev ), RoBERTa achieves\\nstate-of-the-art results on all 9 of the GLUE\\ntask development sets. Crucially, RoBERTa uses\\nthe same masked language modeling pretrain-\\ning objective and architecture as BERT LARGE , yet\\nconsistently outperforms both BERT LARGE and\\nXLNet LARGE . This raises questions about the rel-\\native importance of model architecture and pre-\\ntraining objective, compared to more mundane de-\\ntails like dataset size and training time that we ex-\\nplore in this work.\\nIn the second setting ( ensembles, test ), we\\nsubmit RoBERTa to the GLUE leaderboard and\\nachieve state-of-the-art results on 4 out of 9 tasks\\nand the highest average score to date. This is espe-\\ncially exciting because RoBERTa does not depend\\non multi-task ﬁnetuning, unlike most of the other\\ntop submissions. We expect future work may fur-\\nther improve these results by incorporating more\\nsophisticated multi-task ﬁnetuning procedures.\\n5.2 SQuAD Results\\nWe adopt a much simpler approach for SQuAD\\ncompared to past work. In particular, while\\nboth BERT ( Devlin et al. ,2019 ) and XL-\\nNet ( Yang et al. ,2019 ) augment their training data\\nwith additional QA datasets, we only ﬁnetune\\nRoBERTa using the provided SQuAD training\\ndata .Yang et al. (2019 ) also employed a custom\\nlayer-wise learning rate schedule to ﬁnetune\\nresults could potentially be improved by augmenting this wi th\\nadditional pronoun disambiguation datasets.ModelSQuAD 1.1 SQuAD 2.0\\nEM F1 EM F1\\nSingle models on dev, w/o data augmentation\\nBERT LARGE 84.1 90.9 79.0 81.8\\nXLNet LARGE 89.0 94.5 86.1 88.8\\nRoBERTa 88.9 94.6 86.5 89.4\\nSingle models on test (as of July 25, 2019)\\nXLNet LARGE 86.3†89.1†\\nRoBERTa 86.8 89.8\\nXLNet + SG-Net Veriﬁer 87.0†89.9†\\nTable 6: Results on SQuAD. †indicates results that de-\\npend on additional external training data. RoBERTa\\nuses only the provided SQuAD data in both dev and\\ntest settings. BERT LARGE and XLNet LARGE results are\\nfrom Devlin et al. (2019 ) and Yang et al. (2019 ), re-\\nspectively.\\nXLNet, while we use the same learning rate for\\nall layers.\\nFor SQuAD v1.1 we follow the same ﬁnetun-\\ning procedure as Devlin et al. (2019 ). For SQuAD\\nv2.0, we additionally classify whether a given\\nquestion is answerable; we train this classiﬁer\\njointly with the span predictor by summing the\\nclassiﬁcation and span loss terms.\\nResults We present our results in Table 6. On\\nthe SQuAD v1.1 development set, RoBERTa\\nmatches the state-of-the-art set by XLNet. On the\\nSQuAD v2.0 development set, RoBERTa sets a\\nnew state-of-the-art, improving over XLNet by 0.4\\npoints (EM) and 0.6 points (F1).\\nWe also submit RoBERTa to the public SQuAD\\n2.0 leaderboard and evaluate its performance rel-\\native to other systems. Most of the top systems\\nbuild upon either BERT ( Devlin et al. ,2019 ) or\\nXLNet ( Yang et al. ,2019 ), both of which rely on\\nadditional external training data. In contrast, our\\nsubmission does not use any additional data.\\nOur single RoBERTa model outperforms all but\\none of the single model submissions, and is the\\ntop scoring system among those that do not rely\\non data augmentation.\\n5.3 RACE Results\\nIn RACE, systems are provided with a passage of\\ntext, an associated question, and four candidate an-\\nswers. Systems are required to classify which of\\nthe four candidate answers is correct.\\nWe modify RoBERTa for this task by concate-Model Accuracy Middle High\\nSingle models on test (as of July 25, 2019)\\nBERT LARGE 72.0 76.6 70.1\\nXLNet LARGE 81.7 85.4 80.2\\nRoBERTa 83.2 86.5 81.3\\nTable 7: Results on the RACE test set. BERT LARGE and\\nXLNet LARGE results are from Yang et al. (2019 ).\\nnating each candidate answer with the correspond-\\ning question and passage. We then encode each of\\nthese four sequences and pass the resulting [CLS]\\nrepresentations through a fully-connected layer,\\nwhich is used to predict the correct answer. We\\ntruncate question-answer pairs that are longer than\\n128 tokens and, if needed, the passage so that the\\ntotal length is at most 512 tokens.\\nResults on the RACE test sets are presented in\\nTable 7. RoBERTa achieves state-of-the-art results\\non both middle-school and high-school settings.\\n6 Related Work\\nPretraining methods have been designed\\nwith different training objectives, includ-\\ning language modeling ( Dai and Le ,2015 ;\\nPeters et al. ,2018 ;Howard and Ruder ,2018 ),\\nmachine translation ( McCann et al. ,2017 ), and\\nmasked language modeling ( Devlin et al. ,2019 ;\\nLample and Conneau ,2019 ). Many recent\\npapers have used a basic recipe of ﬁnetuning\\nmodels for each end task ( Howard and Ruder ,\\n2018 ;Radford et al. ,2018 ), and pretraining\\nwith some variant of a masked language model\\nobjective. However, newer methods have\\nimproved performance by multi-task ﬁne tun-\\ning ( Dong et al. ,2019 ), incorporating entity\\nembeddings ( Sun et al. ,2019 ), span predic-\\ntion ( Joshi et al. ,2019 ), and multiple variants\\nof autoregressive pretraining ( Song et al. ,2019 ;\\nChan et al. ,2019 ;Yang et al. ,2019 ). Perfor-\\nmance is also typically improved by training\\nbigger models on more data ( Devlin et al. ,\\n2019 ;Baevski et al. ,2019 ;Yang et al. ,2019 ;\\nRadford et al. ,2019 ). Our goal was to replicate,\\nsimplify, and better tune the training of BERT,\\nas a reference point for better understanding the\\nrelative performance of all of these methods.7 Conclusion\\nWe carefully evaluate a number of design de-\\ncisions when pretraining BERT models. We\\nﬁnd that performance can be substantially im-\\nproved by training the model longer, with bigger\\nbatches over more data; removing the next sen-\\ntence prediction objective; training on longer se-\\nquences; and dynamically changing the masking\\npattern applied to the training data. Our improved\\npretraining procedure, which we call RoBERTa,\\nachieves state-of-the-art results on GLUE, RACE\\nand SQuAD, without multi-task ﬁnetuning for\\nGLUE or additional data for SQuAD. These re-\\nsults illustrate the importance of these previ-\\nously overlooked design decisions and suggest\\nthat BERT’s pretraining objective remains com-\\npetitive with recently proposed alternatives.\\nWe additionally use a novel dataset,\\nCC-N EWS, and release our models and\\ncode for pretraining and ﬁnetuning at:\\nhttps://github.com/pytorch/fairseq .\\nReferences\\nEneko Agirre, Llu’is M‘arquez, and Richard Wicen-\\ntowski, editors. 2007. Proceedings of the Fourth\\nInternational Workshop on Semantic Evaluations\\n(SemEval-2007) .\\nAlexei Baevski, Sergey Edunov, Yinhan Liu, Luke\\nZettlemoyer, and Michael Auli. 2019. Cloze-\\ndriven pretraining of self-attention networks. arXiv\\npreprint arXiv:1903.07785 .\\nRoy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro,\\nDanilo Giampiccolo, Bernardo Magnini, and Idan\\nSzpektor. 2006. The second PASCAL recognising\\ntextual entailment challenge. In Proceedings of the\\nsecond PASCAL challenges workshop on recognis-\\ning textual entailment .\\nLuisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo\\nGiampiccolo, and Bernardo Magnini. 2009. The\\nﬁfth PASCAL recognizing textual entailment chal-\\nlenge.\\nSamuel R Bowman, Gabor Angeli, Christopher Potts,\\nand Christopher D Manning. 2015. A large anno-\\ntated corpus for learning natural language inference.\\nInEmpirical Methods in Natural Language Process-\\ning (EMNLP) .\\nWilliam Chan, Nikita Kitaev, Kelvin Guu, Mitchell\\nStern, and Jakob Uszkoreit. 2019. KERMIT: Gener-\\native insertion-based modeling for sequences. arXiv\\npreprint arXiv:1906.01604 .Ido Dagan, Oren Glickman, and Bernardo Magnini.\\n2006. The PASCAL recognising textual entailment\\nchallenge. In Machine learning challenges. evalu-\\nating predictive uncertainty, visual object classiﬁca-\\ntion, and recognising tectual entailment .\\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\\nsequence learning. In Advances in Neural Informa-\\ntion Processing Systems (NIPS) .\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\\nKristina Toutanova. 2019. BERT: Pre-training of\\ndeep bidirectional transformers for language under-\\nstanding. In North American Association for Com-\\nputational Linguistics (NAACL) .\\nWilliam B Dolan and Chris Brockett. 2005. Auto-\\nmatically constructing a corpus of sentential para-\\nphrases. In Proceedings of the International Work-\\nshop on Paraphrasing .\\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei,\\nXiaodong Liu, Yu Wang, Jianfeng Gao, Ming\\nZhou, and Hsiao-Wuen Hon. 2019. Uniﬁed\\nlanguage model pre-training for natural language\\nunderstanding and generation. arXiv preprint\\narXiv:1905.03197 .\\nDanilo Giampiccolo, Bernardo Magnini, Ido Dagan,\\nand Bill Dolan. 2007. The third PASCAL recog-\\nnizing textual entailment challenge. In Proceedings\\nof the ACL-PASCAL workshop on textual entailment\\nand paraphrasing .\\nAaron Gokaslan and Vanya Cohen. 2019. Openweb-\\ntext corpus. http://web.archive.org/\\nsave/http://Skylion007.github.io/\\nOpenWebTextCorpus .\\nFelix Hamborg, Norman Meuschke, Corinna Bre-\\nitinger, and Bela Gipp. 2017. news-please: A\\ngeneric news crawler and extractor. In Proceedings\\nof the 15th International Symposium of Information\\nScience .\\nDan Hendrycks and Kevin Gimpel. 2016. Gaus-\\nsian error linear units (gelus). arXiv preprint\\narXiv:1606.08415 .\\nMatthew Honnibal and Ines Montani. 2017. spaCy 2:\\nNatural language understanding with Bloom embed-\\ndings, convolutional neural networks and incremen-\\ntal parsing. To appear.\\nJeremy Howard and Sebastian Ruder. 2018. Universal\\nlanguage model ﬁne-tuning for text classiﬁcation.\\narXiv preprint arXiv:1801.06146 .\\nShankar Iyer, Nikhil Dandekar, and Kornl Cser-\\nnai. 2016. First quora dataset release: Question\\npairs.https://data.quora.com/First-\\nQuora-Dataset-Release-Question-\\nPairs .Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S.\\nWeld, Luke Zettlemoyer, and Omer Levy. 2019.\\nSpanBERT: Improving pre-training by repre-\\nsenting and predicting spans. arXiv preprint\\narXiv:1907.10529 .\\nDiederik Kingma and Jimmy Ba. 2015. Adam: A\\nmethod for stochastic optimization. In International\\nConference on Learning Representations (ICLR) .\\nVid Kocijan, Ana-Maria Cretu, Oana-Maria Camburu,\\nYordan Yordanov, and Thomas Lukasiewicz. 2019.\\nA surprisingly robust trick for winograd schema\\nchallenge. arXiv preprint arXiv:1905.06290 .\\nGuokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang,\\nand Eduard Hovy. 2017. Race: Large-scale reading\\ncomprehension dataset from examinations. arXiv\\npreprint arXiv:1704.04683 .\\nGuillaume Lample and Alexis Conneau. 2019. Cross-\\nlingual language model pretraining. arXiv preprint\\narXiv:1901.07291 .\\nHector J Levesque, Ernest Davis, and Leora Morgen-\\nstern. 2011. The Winograd schema challenge. In\\nAAAI Spring Symposium: Logical Formalizations of\\nCommonsense Reasoning .\\nXiaodong Liu, Pengcheng He, Weizhu Chen, and\\nJianfeng Gao. 2019a. Improving multi-task deep\\nneural networks via knowledge distillation for\\nnatural language understanding. arXiv preprint\\narXiv:1904.09482 .\\nXiaodong Liu, Pengcheng He, Weizhu Chen, and Jian-\\nfeng Gao. 2019b. Multi-task deep neural networks\\nfor natural language understanding. arXiv preprint\\narXiv:1901.11504 .\\nBryan McCann, James Bradbury, Caiming Xiong, and\\nRichard Socher. 2017. Learned in translation: Con-\\ntextualized word vectors. In Advances in Neural In-\\nformation Processing Systems (NIPS) , pages 6297–\\n6308.\\nPaulius Micikevicius, Sharan Narang, Jonah Alben,\\nGregory Diamos, Erich Elsen, David Garcia, Boris\\nGinsburg, Michael Houston, Oleksii Kuchaiev,\\nGanesh Venkatesh, and Hao Wu. 2018. Mixed preci-\\nsion training. In International Conference on Learn-\\ning Representations .\\nSebastian Nagel. 2016. Cc-news. http:\\n//web.archive.org/save/http:\\n//commoncrawl.org/2016/10/news-\\ndataset-available .\\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela\\nFan, Sam Gross, Nathan Ng, David Grangier, and\\nMichael Auli. 2019. FAIRSEQ : A fast, exten-\\nsible toolkit for sequence modeling. In North\\nAmerican Association for Computational Linguis-\\ntics (NAACL): System Demonstrations .Myle Ott, Sergey Edunov, David Grangier, and\\nMichael Auli. 2018. Scaling neural machine trans-\\nlation. In Proceedings of the Third Conference on\\nMachine Translation (WMT) .\\nAdam Paszke, Sam Gross, Soumith Chintala, Gre-\\ngory Chanan, Edward Yang, Zachary DeVito, Zem-\\ning Lin, Alban Desmaison, Luca Antiga, and Adam\\nLerer. 2017. Automatic differentiation in PyTorch.\\nInNIPS Autodiff Workshop .\\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\\nGardner, Christopher Clark, Kenton Lee, and Luke\\nZettlemoyer. 2018. Deep contextualized word repre-\\nsentations. In North American Association for Com-\\nputational Linguistics (NAACL) .\\nAlec Radford, Karthik Narasimhan, Time Salimans,\\nand Ilya Sutskever. 2018. Improving language un-\\nderstanding with unsupervised learning. Technical\\nreport, OpenAI.\\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\\nDario Amodei, and Ilya Sutskever. 2019. Language\\nmodels are unsupervised multitask learners. Techni-\\ncal report, OpenAI.\\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018.\\nKnow what you don’t know: Unanswerable ques-\\ntions for squad. In Association for Computational\\nLinguistics (ACL) .\\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\\nPercy Liang. 2016. SQuAD: 100,000+ questions for\\nmachine comprehension of text. In Empirical Meth-\\nods in Natural Language Processing (EMNLP) .\\nRico Sennrich, Barry Haddow, and Alexandra Birch.\\n2016. Neural machine translation of rare words with\\nsubword units. In Association for Computational\\nLinguistics (ACL) , pages 1715–1725.\\nRichard Socher, Alex Perelygin, Jean Wu, Jason\\nChuang, Christopher D Manning, Andrew Ng, and\\nChristopher Potts. 2013. Recursive deep models\\nfor semantic compositionality over a sentiment tree-\\nbank. In Empirical Methods in Natural Language\\nProcessing (EMNLP) .\\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and\\nTie-Yan Liu. 2019. MASS: Masked sequence\\nto sequence pre-training for language generation.\\nInInternational Conference on Machine Learning\\n(ICML) .\\nYu Stephanie Sun, Shuohuan Wang, Yukun Li, Shikun\\nFeng, Xuyi Chen, Han Zhang, Xinlun Tian, Danxi-\\nang Zhu, Hao Tian, and Hua Wu. 2019. ERNIE: En-\\nhanced representation through knowledge integra-\\ntion. arXiv preprint arXiv:1904.09223 .\\nTrieu H Trinh and Quoc V Le. 2018. A simple\\nmethod for commonsense reasoning. arXiv preprint\\narXiv:1806.02847 .Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\\nKaiser, and Illia Polosukhin. 2017. Attention is all\\nyou need. In Advances in neural information pro-\\ncessing systems .\\nAlex Wang, Yada Pruksachatkun, Nikita Nangia,\\nAmanpreet Singh, Julian Michael, Felix Hill, Omer\\nLevy, and Samuel R. Bowman. 2019a. SuperGLUE:\\nA stickier benchmark for general-purpose language\\nunderstanding systems. arXiv preprint 1905.00537 .\\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\\nHill, Omer Levy, and Samuel R. Bowman. 2019b.\\nGLUE: A multi-task benchmark and analysis plat-\\nform for natural language understanding. In Inter-\\nnational Conference on Learning Representations\\n(ICLR) .\\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bow-\\nman. 2018. Neural network acceptability judg-\\nments. arXiv preprint 1805.12471 .\\nAdina Williams, Nikita Nangia, and Samuel Bowman.\\n2018. A broad-coverage challenge corpus for sen-\\ntence understanding through inference. In North\\nAmerican Association for Computational Linguis-\\ntics (NAACL) .\\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\\nbonell, Ruslan Salakhutdinov, and Quoc V Le.\\n2019. Xlnet: Generalized autoregressive pretrain-\\ning for language understanding. arXiv preprint\\narXiv:1906.08237 .\\nYang You, Jing Li, Jonathan Hseu, Xiaodan Song,\\nJames Demmel, and Cho-Jui Hsieh. 2019. Reduc-\\ning bert pre-training time from 3 days to 76 minutes.\\narXiv preprint arXiv:1904.00962 .\\nRowan Zellers, Ari Holtzman, Hannah Rashkin,\\nYonatan Bisk, Ali Farhadi, Franziska Roesner, and\\nYejin Choi. 2019. Defending against neural fake\\nnews. arXiv preprint arXiv:1905.12616 .\\nYukun Zhu, Ryan Kiros, Richard Zemel, Ruslan\\nSalakhutdinov, Raquel Urtasun, Antonio Torralba,\\nand Sanja Fidler. 2015. Aligning books and movies:\\nTowards story-like visual explanations by watch-\\ning movies and reading books. In arXiv preprint\\narXiv:1506.06724 .\\nAppendix for “RoBERTa: A Robustly\\nOptimized BERT Pretraining Approach”\\nA Full results on GLUE\\nIn Table 8we present the full set of development\\nset results for RoBERTa. We present results for\\naLARGE conﬁguration that follows BERT LARGE ,\\nas well as a BASE conﬁguration that follows\\nBERT BASE.B Pretraining Hyperparameters\\nTable 9describes the hyperparameters for pre-\\ntraining of RoBERTa LARGE and RoBERTa BASE\\nC Finetuning Hyperparameters\\nFinetuning hyperparameters for RACE, SQuAD\\nand GLUE are given in Table 10. We select the\\nbest hyperparameter values based on the median\\nof 5 random seeds for each task.MNLI QNLI QQP RTE SST MRPC CoLA STS\\nRoBERTa BASE\\n+ all data + 500k steps 87.6 92.8 91.9 78.7 94.8 90.2 63.6 91.2\\nRoBERTa LARGE\\nwith B OOKS + W IKI 89.0 93.9 91.9 84.5 95.3 90.2 66.3 91.6\\n+ additional data ( §3.2) 89.3 94.0 92.0 82.7 95.6 91.4 66.1 92.2\\n+ pretrain longer 300k 90.0 94.5 92.2 83.3 96.1 91.1 67.4 92.3\\n+ pretrain longer 500k 90.2 94.7 92.2 86.6 96.4 90.9 68.0 92.4\\nTable 8: Development set results on GLUE tasks for various co nﬁgurations of RoBERTa.\\nHyperparam RoBERTa LARGE RoBERTa BASE\\nNumber of Layers 24 12\\nHidden size 1024 768\\nFFN inner hidden size 4096 3072\\nAttention heads 16 12\\nAttention head size 64 64\\nDropout 0.1 0.1\\nAttention Dropout 0.1 0.1\\nWarmup Steps 30k 24k\\nPeak Learning Rate 4e-4 6e-4\\nBatch Size 8k 8k\\nWeight Decay 0.01 0.01\\nMax Steps 500k 500k\\nLearning Rate Decay Linear Linear\\nAdamǫ 1e-6 1e-6\\nAdamβ1 0.9 0.9\\nAdamβ2 0.98 0.98\\nGradient Clipping 0.0 0.0\\nTable 9: Hyperparameters for pretraining RoBERTa LARGE and RoBERTa BASE.\\nHyperparam RACE SQuAD GLUE\\nLearning Rate 1e-5 1.5e-5 {1e-5, 2e-5, 3e-5 }\\nBatch Size 16 48 {16, 32}\\nWeight Decay 0.1 0.01 0.1\\nMax Epochs 4 2 10\\nLearning Rate Decay Linear Linear Linear\\nWarmup ratio 0.06 0.06 0.06\\nTable 10: Hyperparameters for ﬁnetuning RoBERTa LARGE on RACE, SQuAD and GLUE.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46762e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks 61\n",
      "============================\n",
      "arXiv:1907.11692v1  [cs.CL]  26 Jul 2019RoBERTa: A Robustly Optimized BERT Pretraining Approach\n",
      "Yinhan Liu∗§Myle Ott∗§Naman Goyal∗§Jingfei Du∗§Mandar Joshi†\n",
      "Danqi Chen§Omer Levy§Mike Lewis§Luke Zettlemoyer†§Veselin Stoyanov§\n",
      "†Paul G. Allen School of Computer Science & Engineering,\n",
      "University of Washington, Seattle, WA\n",
      "{mandar90,lsz }@cs.washington.edu\n",
      "§Facebook AI\n",
      "{yinhanliu,myleott,naman,jingfeidu,\n",
      "danqi,omerlevy,mikelewis,lsz,ves }@fb.com\n",
      "Abstract\n",
      "Language model pretraining has led to sig-\n",
      "niﬁcant performance gains but careful com-\n",
      "parison between different approaches is chal-\n",
      "lenging. Training is computationally expen-\n",
      "sive, often done on private datasets of different\n",
      "sizes, and, as we will show, hyperparameter\n",
      "choices have signiﬁcant impact on the ﬁnal re-\n",
      "sults. We present a replication study of BERT\n",
      "pretraining ( Devlin et al. ,2019 ) that carefully\n",
      "measures the impact of many key hyperparam-\n",
      "eters and training data size. We ﬁnd that BERT\n"
     ]
    }
   ],
   "source": [
    "splitter = CharacterTextSplitter(\n",
    "    separator = \"\\n\",\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap  = 200,\n",
    "    length_function = len,\n",
    ")\n",
    "text_chunks = splitter.split_text(pdf_text)\n",
    "print(f\"Total chunks {len(text_chunks)}\")\n",
    "print(\"============================\")\n",
    "print(text_chunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8407b263",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "embedding_vectors = FAISS.from_texts(text_chunks, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32db2f4",
   "metadata": {},
   "source": [
    "## Extracting Information from Research Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68f26edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "qa_chain = load_qa_chain(OpenAI(), \n",
    "                         chain_type=\"stuff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67087f51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The datasets used in this paper include Book Corpus and English Wikipedia, CC-NEWS, OpenWebText, and Stories.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Can you give me a list of datasets used in this paper?\"\n",
    "\n",
    "research_paper = embedding_vectors.similarity_search(question)\n",
    "\n",
    "qa_chain.run(input_documents = research_paper, \n",
    "             question = question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "75c96e35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The paper found that RoBERTa trained for 500K steps outperformed XLNet LARGE across most tasks on GLUE, SQuaD, and RACE. On GLUE, RoBERTa achieved state-of-the-art results on all 9 of the tasks in the first setting (single-task, dev) and 4 out of 9 tasks in the second setting (ensembles, test), and had the highest average score to date. On SQuAD and RACE, RoBERTa trained for 500K steps achieved significant gains in downstream task performance.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"Can you summarize the benchmark results from the paper?\"\n",
    "\n",
    "research_paper = embedding_vectors.similarity_search(question)\n",
    "\n",
    "qa_chain.run(input_documents = research_paper, \n",
    "             question = question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5ac80abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(questions):\n",
    "    \n",
    "    answers = []\n",
    "    for question in questions:\n",
    "        \n",
    "        research_paper = embedding_vectors.similarity_search(question)\n",
    "\n",
    "        answer = qa_chain.run(input_documents = research_paper, \n",
    "                 question = question)\n",
    "        \n",
    "        answers.append(answer)\n",
    "    \n",
    "    return answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8a842959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Question: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">: Can you give me a list of datasets used in this paper? </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQuestion: \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m: Can you give me a list of datasets used in this paper? \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Answer:</span>  The datasets used in this paper are Book Corpus plus English Wikipedia, CC-NEWS, OpenWebText, STORIES, and\n",
       "SQuAD.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mAnswer:\u001b[0m  The datasets used in this paper are Book Corpus plus English Wikipedia, CC-NEWS, OpenWebText, STORIES, and\n",
       "SQuAD.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">==================================================\n",
       "</pre>\n"
      ],
      "text/plain": [
       "==================================================\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Question: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">: What are the evaluation metrics used in the paper? </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQuestion: \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m: What are the evaluation metrics used in the paper? \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Answer:</span>  The authors evaluate their pretrained models on downstream tasks using the GLUE benchmark, SQuAD V1.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> and \n",
       "V2.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, and RACE. The evaluation metrics used are median development set results for each task over five random \n",
       "initializations <span style=\"font-weight: bold\">(</span>for GLUE<span style=\"font-weight: bold\">)</span>, and accuracy and F1 scores <span style=\"font-weight: bold\">(</span>for SQuAD and RACE<span style=\"font-weight: bold\">)</span>.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mAnswer:\u001b[0m  The authors evaluate their pretrained models on downstream tasks using the GLUE benchmark, SQuAD V1.\u001b[1;36m1\u001b[0m and \n",
       "V2.\u001b[1;36m0\u001b[0m, and RACE. The evaluation metrics used are median development set results for each task over five random \n",
       "initializations \u001b[1m(\u001b[0mfor GLUE\u001b[1m)\u001b[0m, and accuracy and F1 scores \u001b[1m(\u001b[0mfor SQuAD and RACE\u001b[1m)\u001b[0m.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">==================================================\n",
       "</pre>\n"
      ],
      "text/plain": [
       "==================================================\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Question: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">: Can you summarize the benchmark results from the paper? </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mQuestion: \u001b[0m\u001b[1;36m3\u001b[0m\u001b[1m: Can you summarize the benchmark results from the paper? \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Answer:</span>  The paper reports that RoBERTa trained for 500K steps across the five datasets introduced in Section <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3.2</span> \n",
       "outperformed XLNet LARGE across most GLUE tasks, and achieved state-of-the-art results on four out of nine GLUE \n",
       "tasks and the highest average score to date, without using multi-task finetuning. On SQuAD, RoBERTa achieved \n",
       "further improvements in performance. On RACE, RoBERTa achieved state-of-the-art results.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mAnswer:\u001b[0m  The paper reports that RoBERTa trained for 500K steps across the five datasets introduced in Section \u001b[1;36m3.2\u001b[0m \n",
       "outperformed XLNet LARGE across most GLUE tasks, and achieved state-of-the-art results on four out of nine GLUE \n",
       "tasks and the highest average score to date, without using multi-task finetuning. On SQuAD, RoBERTa achieved \n",
       "further improvements in performance. On RACE, RoBERTa achieved state-of-the-art results.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">==================================================\n",
       "</pre>\n"
      ],
      "text/plain": [
       "==================================================\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rich import print\n",
    "\n",
    "questions = [\"Can you give me a list of datasets used in this paper?\",\n",
    "             \"What are the evaluation metrics used in the paper?\",\n",
    "             \"Can you summarize the benchmark results from the paper?\"]\n",
    "\n",
    "answers = get_answer(questions)\n",
    "\n",
    "for i in range(len(questions)):\n",
    "    print(f\"[bold]Question: {i+1}: {questions[i]} [/bold]\")\n",
    "    print(f\"[bold]Answer:[/bold] {answers[i]}\")\n",
    "    print(\"==================================================\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad893b3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
